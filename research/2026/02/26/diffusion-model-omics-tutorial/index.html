<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Diffusion Model 기초 이론과 오믹스 데이터 적용 가이드 — 놀놀할할</title>
  <meta name="description" content="
  이 포스트는 Diffusion Model의 기본 이론을 수학적으로 설명하고, 오믹스(omics) 데이터에 적용하는 방법을 PyTorch 예제 코드와 함께 다룬다. 바이오인포매틱스 연구자가 생성 모델을 이해하고 활용하는 데 도움이 되기를 바란다.

">
  
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Noto+Sans+KR:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  
  <!-- Styles -->
  <link rel="stylesheet" href="/assets/css/main.css">
  
  <!-- SEO -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Diffusion Model 기초 이론과 오믹스 데이터 적용 가이드 | 놀놀할할</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Diffusion Model 기초 이론과 오믹스 데이터 적용 가이드" />
<meta name="author" content="Sungwon" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="이 포스트는 Diffusion Model의 기본 이론을 수학적으로 설명하고, 오믹스(omics) 데이터에 적용하는 방법을 PyTorch 예제 코드와 함께 다룬다. 바이오인포매틱스 연구자가 생성 모델을 이해하고 활용하는 데 도움이 되기를 바란다." />
<meta property="og:description" content="이 포스트는 Diffusion Model의 기본 이론을 수학적으로 설명하고, 오믹스(omics) 데이터에 적용하는 방법을 PyTorch 예제 코드와 함께 다룬다. 바이오인포매틱스 연구자가 생성 모델을 이해하고 활용하는 데 도움이 되기를 바란다." />
<link rel="canonical" href="https://jsungwon.github.io/research/2026/02/26/diffusion-model-omics-tutorial/" />
<meta property="og:url" content="https://jsungwon.github.io/research/2026/02/26/diffusion-model-omics-tutorial/" />
<meta property="og:site_name" content="놀놀할할" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-02-26T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Diffusion Model 기초 이론과 오믹스 데이터 적용 가이드" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Sungwon"},"dateModified":"2026-02-26T00:00:00+00:00","datePublished":"2026-02-26T00:00:00+00:00","description":"이 포스트는 Diffusion Model의 기본 이론을 수학적으로 설명하고, 오믹스(omics) 데이터에 적용하는 방법을 PyTorch 예제 코드와 함께 다룬다. 바이오인포매틱스 연구자가 생성 모델을 이해하고 활용하는 데 도움이 되기를 바란다.","headline":"Diffusion Model 기초 이론과 오믹스 데이터 적용 가이드","mainEntityOfPage":{"@type":"WebPage","@id":"https://jsungwon.github.io/research/2026/02/26/diffusion-model-omics-tutorial/"},"url":"https://jsungwon.github.io/research/2026/02/26/diffusion-model-omics-tutorial/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DWB335C28N"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-DWB335C28N');
  </script>
</head>
<body>
  <header class="site-header">
  <div class="header-inner">
    <a href="/" class="site-logo">
      <svg class="logo-svg" width="26" height="26" viewBox="0 0 28 28" fill="none" aria-hidden="true">
        <path d="M9 2C20 5 20 10 14 14C8 18 8 23 19 26" stroke="#2563eb" stroke-width="2.5" stroke-linecap="round" fill="none"/>
        <path d="M19 2C8 5 8 10 14 14C20 18 20 23 9 26" stroke="#7c3aed" stroke-width="2.5" stroke-linecap="round" fill="none"/>
        <line x1="11" y1="6" x2="17" y2="6" stroke="#93c5fd" stroke-width="1.5" stroke-linecap="round"/>
        <line x1="12" y1="10" x2="16" y2="10" stroke="#c4b5fd" stroke-width="1.5" stroke-linecap="round"/>
        <line x1="12" y1="18" x2="16" y2="18" stroke="#93c5fd" stroke-width="1.5" stroke-linecap="round"/>
        <line x1="11" y1="22" x2="17" y2="22" stroke="#c4b5fd" stroke-width="1.5" stroke-linecap="round"/>
      </svg>
      <span class="logo-text">놀놀할할</span>
    </a>

    <nav class="site-nav">
      <a href="/research/" class="nav-link nav-link--active">Research</a>
      <a href="/tech/" class="nav-link ">Tech</a>
      <a href="/trip/" class="nav-link ">Trip</a>
      <a href="/food/" class="nav-link ">Food</a>
      <a href="/tags/" class="nav-link ">Tags</a>
      <a href="/about/" class="nav-link ">About</a>
    </nav>

    <div class="header-actions">
      <button class="search-toggle" aria-label="검색 열기">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="11" cy="11" r="8"/>
          <line x1="21" y1="21" x2="16.65" y2="16.65"/>
        </svg>
      </button>
      <button class="nav-toggle" aria-label="메뉴 열기">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
  </div>
</header>

<div class="search-overlay" id="searchOverlay">
  <div class="search-modal">
    <div class="search-modal__header">
      <div class="search-input-wrap">
        <svg class="search-input-icon" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round">
          <circle cx="11" cy="11" r="8"/>
          <line x1="21" y1="21" x2="16.65" y2="16.65"/>
        </svg>
        <input
          type="text"
          class="search-input"
          id="searchInput"
          placeholder="제목 또는 태그로 검색..."
          autocomplete="off"
        >
        <kbd class="search-kbd">ESC</kbd>
      </div>
    </div>
    <div class="search-modal__body" id="searchResults">
      <div class="search-empty">
        <p class="search-empty__text">검색어를 입력하세요</p>
      </div>
    </div>
  </div>
</div>

<script src="/assets/js/search.js"></script>


<script>
  const toggle = document.querySelector('.nav-toggle');
  const nav = document.querySelector('.site-nav');
  toggle?.addEventListener('click', () => {
    nav.classList.toggle('is-open');
    toggle.classList.toggle('is-active');
  });
</script>

  
  <main class="site-main">
    <article class="post-full">
  <header class="post-header">
    <div class="post-meta">
      <span class="post-category post-category--research">
        🧬 Research
      </span>
      <time datetime="2026-02-26T00:00:00+00:00">
        2026년 02월 26일
      </time>
    </div>
    <h1 class="post-title">Diffusion Model 기초 이론과 오믹스 데이터 적용 가이드<span class="badge-ai">AI Generated</span></h1>
    
    <p class="post-subtitle">DDPM의 수학적 기초부터 유전자 발현 데이터를 활용한 실전 구현까지</p>
    
    
    <div class="post-tags">
      
      <a href="/tags/#diffusion-model" class="tag tag--link">#diffusion model</a>
      
      <a href="/tags/#deep-learning" class="tag tag--link">#deep learning</a>
      
      <a href="/tags/#omics" class="tag tag--link">#omics</a>
      
      <a href="/tags/#single-cell" class="tag tag--link">#single-cell</a>
      
      <a href="/tags/#generative-model" class="tag tag--link">#generative model</a>
      
      <a href="/tags/#tutorial" class="tag tag--link">#tutorial</a>
      
    </div>
    
  </header>

  

  <div class="post-content">
    <blockquote>
  <p>이 포스트는 Diffusion Model의 기본 이론을 수학적으로 설명하고, 오믹스(omics) 데이터에 적용하는 방법을 PyTorch 예제 코드와 함께 다룬다. 바이오인포매틱스 연구자가 생성 모델을 이해하고 활용하는 데 도움이 되기를 바란다.</p>
</blockquote>

<hr />

<h2 id="왜-diffusion-model인가">왜 Diffusion Model인가?</h2>

<p>생성 모델(generative model)은 데이터의 분포를 학습하여 새로운 샘플을 생성하는 모델이다. 이미지 생성에서 시작된 확산 모델(diffusion model)은 이제 단일세포 전사체(single-cell transcriptomics), 단백질체(proteomics), 약물 반응 예측 등 생명과학 분야 전반으로 확장되고 있다.</p>

<p>기존 생성 모델들과의 비교:</p>

<table>
  <thead>
    <tr>
      <th>모델</th>
      <th>장점</th>
      <th>단점</th>
      <th>오믹스 적용 사례</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>GAN</strong></td>
      <td>빠른 생성 속도</td>
      <td>학습 불안정, mode collapse</td>
      <td>scGAN</td>
    </tr>
    <tr>
      <td><strong>VAE</strong></td>
      <td>안정적 학습, 잠재 공간 해석</td>
      <td>blurry한 생성 결과</td>
      <td>scVI, PRnet</td>
    </tr>
    <tr>
      <td><strong>Flow</strong></td>
      <td>정확한 likelihood 계산</td>
      <td>구조적 제약</td>
      <td>scFlow</td>
    </tr>
    <tr>
      <td><strong>Diffusion</strong></td>
      <td>높은 생성 품질, 안정적 학습</td>
      <td>느린 생성 속도</td>
      <td>scDiffusion, GDDM</td>
    </tr>
  </tbody>
</table>

<p>Diffusion model이 오믹스 분야에서 주목받는 이유는 명확하다:</p>

<ol>
  <li><strong>학습 안정성</strong>: GAN처럼 generator-discriminator의 균형을 맞출 필요 없음</li>
  <li><strong>고품질 생성</strong>: VAE의 over-smoothing 문제 없이 데이터의 세밀한 분포를 포착</li>
  <li><strong>유연한 조건부 생성</strong>: 세포 유형, 조직, 질병 상태 등 다양한 조건으로 생성 제어 가능</li>
  <li><strong>이론적 기반</strong>: 비평형 열역학(non-equilibrium thermodynamics)에 기초한 탄탄한 수학적 프레임워크</li>
</ol>

<hr />

<h2 id="part-1-diffusion-model-기본-이론">Part 1: Diffusion Model 기본 이론</h2>

<h3 id="핵심-아이디어">핵심 아이디어</h3>

<p>Diffusion model의 핵심은 놀랍도록 단순하다:</p>

<ol>
  <li><strong>순방향 과정(Forward Process)</strong>: 데이터에 점진적으로 가우시안 노이즈(Gaussian noise)를 추가하여 순수 노이즈로 변환</li>
  <li><strong>역방향 과정(Reverse Process)</strong>: 노이즈로부터 원래 데이터를 복원하는 과정을 신경망으로 학습</li>
</ol>

<p>비유하자면, 잉크를 물에 떨어뜨리면 점차 확산(diffusion)되어 균일한 색이 된다. 역방향 과정은 이 균일한 색의 물에서 원래 잉크 방울의 위치를 복원하는 것과 같다.</p>

<h3 id="11-순방향-과정-forward-diffusion-process">1.1 순방향 과정 (Forward Diffusion Process)</h3>

<p>원본 데이터 <strong>x₀</strong>에서 시작하여 <strong>T</strong> 스텝에 걸쳐 점진적으로 가우시안 노이즈를 추가하는 마르코프 체인(Markov chain)이다.</p>

<p>각 스텝에서의 노이즈 추가:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>q(xₜ | xₜ₋₁) = N(xₜ; √(1 - βₜ) · xₜ₋₁, βₜ · I)
</code></pre></div></div>

<p>여기서:</p>
<ul>
  <li><strong>βₜ</strong> : 타임스텝 t에서의 노이즈 스케줄(noise schedule), 0 &lt; β₁ &lt; β₂ &lt; … &lt; βT &lt; 1</li>
  <li><strong>N(μ, σ²I)</strong> : 평균 μ, 분산 σ²I인 가우시안 분포</li>
  <li><strong>I</strong> : 단위 행렬</li>
</ul>

<p>전체 순방향 과정을 한 번에 표현하면 (<code class="language-plaintext highlighter-rouge">reparameterization trick</code>):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>αₜ = 1 - βₜ
ᾱₜ = α₁ · α₂ · ... · αₜ  (cumulative product)

q(xₜ | x₀) = N(xₜ; √ᾱₜ · x₀, (1 - ᾱₜ) · I)
</code></pre></div></div>

<p>이를 통해 <strong>임의의 타임스텝 t에서의 노이즈 데이터를 직접 샘플링</strong>할 수 있다:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>xₜ = √ᾱₜ · x₀ + √(1 - ᾱₜ) · ε,  ε ~ N(0, I)
</code></pre></div></div>

<p>이것이 매우 중요한 성질이다. 학습 시 순차적으로 노이즈를 추가할 필요 없이, 원본 데이터 x₀로부터 어떤 타임스텝의 노이즈 버전이든 한 번에 얻을 수 있다.</p>

<h3 id="12-노이즈-스케줄-noise-schedule">1.2 노이즈 스케줄 (Noise Schedule)</h3>

<p>βₜ의 설계는 모델 성능에 큰 영향을 미친다. 대표적인 스케줄:</p>

<p><strong>Linear Schedule</strong> (Ho et al., 2020):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>βₜ = β₁ + (β_T - β₁) · (t - 1) / (T - 1)
β₁ = 0.0001, β_T = 0.02, T = 1000
</code></pre></div></div>

<p><strong>Cosine Schedule</strong> (Nichol &amp; Dhariwal, 2021):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ᾱₜ = f(t) / f(0),  f(t) = cos((t/T + s) / (1 + s) · π/2)²
</code></pre></div></div>

<p>Cosine schedule은 linear schedule에 비해 초반 스텝에서 정보를 더 오래 보존하여 학습 효율이 높다. 오믹스 데이터는 sparse한 특성이 있어, cosine schedule이 더 적합한 경우가 많다.</p>

<h3 id="13-역방향-과정-reverse-process">1.3 역방향 과정 (Reverse Process)</h3>

<p>역방향 과정은 순수 노이즈 <strong>x_T ~ N(0, I)</strong>에서 시작하여 원본 데이터 <strong>x₀</strong>를 복원한다:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pθ(xₜ₋₁ | xₜ) = N(xₜ₋₁; μθ(xₜ, t), Σθ(xₜ, t))
</code></pre></div></div>

<p>신경망 <strong>εθ</strong>가 각 타임스텝에서 추가된 노이즈를 예측하도록 학습된다. 예측된 노이즈로부터 평균 μθ를 계산:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>μθ(xₜ, t) = 1/√αₜ · (xₜ - βₜ/√(1 - ᾱₜ) · εθ(xₜ, t))
</code></pre></div></div>

<h3 id="14-학습-목적-함수-training-objective">1.4 학습 목적 함수 (Training Objective)</h3>

<p>DDPM(Denoising Diffusion Probabilistic Models, Ho et al. 2020)의 학습은 <strong>단순한 노이즈 예측 MSE 손실</strong>로 귀결된다:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>L_simple = E[‖ε - εθ(xₜ, t)‖²]
         = E[‖ε - εθ(√ᾱₜ · x₀ + √(1 - ᾱₜ) · ε, t)‖²]
</code></pre></div></div>

<p>여기서:</p>
<ul>
  <li><strong>ε ~ N(0, I)</strong>: 실제 추가된 노이즈</li>
  <li><strong>εθ(xₜ, t)</strong>: 신경망이 예측한 노이즈</li>
  <li><strong>t ~ Uniform(1, T)</strong>: 무작위로 선택된 타임스텝</li>
</ul>

<p>학습 과정을 한 눈에 보면:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. 원본 데이터 x₀를 배치에서 샘플링
2. 타임스텝 t를 균등 분포에서 샘플링
3. 가우시안 노이즈 ε ~ N(0, I) 샘플링
4. 노이즈 데이터 계산: xₜ = √ᾱₜ · x₀ + √(1 - ᾱₜ) · ε
5. 신경망으로 노이즈 예측: ε̂ = εθ(xₜ, t)
6. 손실 계산: L = ‖ε - ε̂‖²
7. 역전파로 θ 업데이트
</code></pre></div></div>

<h3 id="15-score-based-관점">1.5 Score-Based 관점</h3>

<p>Song &amp; Ermon (2019)의 score-based generative model 관점에서, 확산 모델은 <strong>스코어 함수(score function)</strong>를 학습하는 것으로 해석할 수 있다:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>스코어 함수: sθ(x) ≈ ∇x log p(x)
</code></pre></div></div>

<p>스코어 함수는 데이터 분포의 로그 확률의 기울기(gradient)로, “데이터 밀도가 높은 방향”을 가리킨다. 노이즈 예측 εθ와 스코어 함수는 다음 관계로 연결된다:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sθ(xₜ, t) = -εθ(xₜ, t) / √(1 - ᾱₜ)
</code></pre></div></div>

<p>즉, <strong>노이즈를 예측하는 것은 곧 스코어 함수를 학습하는 것</strong>과 동치다.</p>

<h3 id="16-sde-통합-프레임워크">1.6 SDE 통합 프레임워크</h3>

<p>Song et al. (ICLR 2021, Outstanding Paper)은 DDPM과 score-based model을 <strong>확률 미분 방정식(SDE)</strong>으로 통합했다. 타임스텝 수를 무한대로 보내면, 이산적 마르코프 체인이 연속 시간 SDE로 수렴한다:</p>

<p><strong>순방향 SDE</strong>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dx = f(x, t)dt + g(t)dw
</code></pre></div></div>

<p><strong>역방향 SDE</strong>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dx = [f(x, t) - g(t)² · ∇x log pₜ(x)]dt + g(t)dw̄
</code></pre></div></div>

<p>여기서 <strong>∇x log pₜ(x)</strong>가 바로 스코어 함수다. 또한 확률적 노이즈 없이 결정론적으로 샘플링하는 <strong>Probability Flow ODE</strong>도 유도된다:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dx = [f(x, t) - ½g(t)² · ∇x log pₜ(x)]dt
</code></pre></div></div>

<p>이 ODE는 DDIM(Denoising Diffusion Implicit Models)의 이론적 기반이 되며, 더 적은 스텝으로 빠른 샘플링을 가능하게 한다.</p>

<hr />

<h2 id="part-2-오믹스-데이터에-대한-diffusion-model">Part 2: 오믹스 데이터에 대한 Diffusion Model</h2>

<h3 id="21-오믹스-데이터의-특수성">2.1 오믹스 데이터의 특수성</h3>

<p>유전자 발현(gene expression) 데이터는 이미지와 근본적으로 다른 특성을 가진다:</p>

<table>
  <thead>
    <tr>
      <th>특성</th>
      <th>이미지</th>
      <th>유전자 발현</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>차원</strong></td>
      <td>고정 해상도 (예: 256×256×3)</td>
      <td>수천~수만 유전자</td>
    </tr>
    <tr>
      <td><strong>공간 구조</strong></td>
      <td>인접 픽셀 간 강한 상관관계</td>
      <td>유전자 간 순서 무관 (비정렬)</td>
    </tr>
    <tr>
      <td><strong>분포</strong></td>
      <td>[0, 255] 연속 정수</td>
      <td>비음수, 고도로 희소(sparse)</td>
    </tr>
    <tr>
      <td><strong>제로 비율</strong></td>
      <td>극소</td>
      <td>60~90% (dropout)</td>
    </tr>
    <tr>
      <td><strong>네트워크</strong></td>
      <td>CNN, ViT 등</td>
      <td>MLP, Transformer</td>
    </tr>
  </tbody>
</table>

<p>이러한 차이점 때문에 이미지용 확산 모델을 오믹스에 직접 적용하면 성능이 좋지 않다. 핵심적인 적응(adaptation) 전략은 다음과 같다:</p>

<h3 id="22-잠재-확산-모델-latent-diffusion-model-접근">2.2 잠재 확산 모델 (Latent Diffusion Model) 접근</h3>

<p>고차원 유전자 발현 데이터에 직접 확산을 적용하는 대신, <strong>잠재 공간(latent space)</strong>에서 확산을 수행하는 전략이 효과적이다:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[유전자 발현 프로파일] → [인코더] → [잠재 표현] → [확산 과정] → [디코더] → [생성된 프로파일]
   ~20,000 차원            128~256 차원                           ~20,000 차원
</code></pre></div></div>

<p><strong>장점</strong>:</p>
<ul>
  <li>차원 축소로 계산 효율 대폭 향상</li>
  <li>인코더가 dropout 노이즈를 제거하여 더 깨끗한 표현 학습</li>
  <li>잠재 공간이 가우시안에 가까운 분포로 정규화되어 확산 과정에 적합</li>
</ul>

<p>대표적인 사례:</p>
<ul>
  <li><strong>scDiffusion</strong> (Luo et al., 2024): SCimilarity 파운데이션 모델을 인코더로 활용, 128차원 잠재 공간</li>
  <li><strong>GDDM</strong> (Gao et al., 2024): VAE 기반 인코더-디코더 + 잠재 공간 확산</li>
</ul>

<h3 id="23-네트워크-아키텍처-선택">2.3 네트워크 아키텍처 선택</h3>

<p>이미지 확산 모델에서는 U-Net이나 DiT(Diffusion Transformer)가 표준이지만, 유전자 발현 데이터에서는 공간 구조가 없으므로 다른 아키텍처가 필요하다:</p>

<p><strong>MLP 기반 디노이징 네트워크</strong> (가장 일반적):</p>
<ul>
  <li>스킵 연결(skip connection)이 있는 다층 퍼셉트론</li>
  <li>타임스텝 임베딩을 각 레이어에 주입</li>
  <li>유전자 간 순서가 무의미하므로 MLP가 자연스러운 선택</li>
</ul>

<p><strong>Transformer 기반</strong>:</p>
<ul>
  <li>유전자를 토큰으로 취급하여 self-attention 적용</li>
  <li>유전자 간 상호작용(gene-gene interaction)을 명시적으로 모델링 가능</li>
  <li>계산 비용이 높지만 표현력이 풍부</li>
</ul>

<h3 id="24-조건부-생성-전략">2.4 조건부 생성 전략</h3>

<p>오믹스 데이터의 조건부 생성은 크게 두 가지 방식으로 이루어진다:</p>

<p><strong>분류기 가이던스 (Classifier Guidance)</strong>:</p>
<ul>
  <li>별도의 분류기를 학습하여 생성 방향을 제어</li>
  <li>장점: 디노이징 네트워크와 독립적으로 학습 가능</li>
  <li>단점: 추가 모델 필요</li>
  <li>예: scDiffusion의 세포 유형 분류기</li>
</ul>

<p><strong>분류기 없는 가이던스 (Classifier-Free Guidance, CFG)</strong>:</p>
<ul>
  <li>조건부/비조건부 생성을 하나의 모델에서 동시에 학습</li>
  <li>학습 시 일정 확률로 조건을 dropout (빈 조건으로 대체)</li>
  <li>추론 시 조건부와 비조건부 예측의 차이를 증폭</li>
  <li>예: cfDiffusion (Zhang et al., 2024)</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ε̂_guided = ε̂_uncond + w · (ε̂_cond - ε̂_uncond)
</code></pre></div></div>

<p>여기서 <strong>w</strong>는 가이던스 강도(guidance scale)로, 크게 할수록 조건에 더 충실한 생성이 이루어진다.</p>

<h3 id="25-주요-오믹스-diffusion-model-비교">2.5 주요 오믹스 Diffusion Model 비교</h3>

<table>
  <thead>
    <tr>
      <th>모델</th>
      <th>연도</th>
      <th>데이터 유형</th>
      <th>잠재 공간</th>
      <th>가이던스</th>
      <th>핵심 특징</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>scDiffusion</strong></td>
      <td>2024</td>
      <td>scRNA-seq</td>
      <td>SCimilarity (128d)</td>
      <td>Classifier</td>
      <td>Gradient Interpolation으로 발달 궤적 생성</td>
    </tr>
    <tr>
      <td><strong>cfDiffusion</strong></td>
      <td>2025</td>
      <td>scRNA-seq</td>
      <td>Autoencoder</td>
      <td>Classifier-free</td>
      <td>별도 분류기 불필요, 다중 속성 조건부 생성</td>
    </tr>
    <tr>
      <td><strong>scVAEDer</strong></td>
      <td>2025</td>
      <td>scRNA-seq</td>
      <td>VAE + Diffusion</td>
      <td>하이브리드</td>
      <td>벡터 산술로 섭동 반응 예측</td>
    </tr>
    <tr>
      <td><strong>scLDM</strong></td>
      <td>2025</td>
      <td>scRNA-seq</td>
      <td>Transformer VAE</td>
      <td>Classifier-free</td>
      <td>유전자 교환가능성(exchangeability) 보장, Flow Matching 손실</td>
    </tr>
    <tr>
      <td><strong>DCM</strong></td>
      <td>2026</td>
      <td>scRNA-seq</td>
      <td>직접 (이산)</td>
      <td>Conditional</td>
      <td>이산 확산으로 카운트 데이터 직접 모델링</td>
    </tr>
  </tbody>
</table>

<p><strong>최근 동향</strong>: 2025년 이후로 <strong>Classifier-free guidance</strong>가 주류가 되고 있으며(cfDiffusion), 유전자 발현의 이산적(discrete) 특성을 직접 모델링하는 <strong>DCM</strong>(2026)이 기존 연속 확산 모델 대비 MMD² RBF에서 5배 향상을 보고했다.</p>

<hr />

<h2 id="part-3-pytorch로-구현하는-오믹스-diffusion-model">Part 3: PyTorch로 구현하는 오믹스 Diffusion Model</h2>

<p>이론을 코드로 옮겨보자. 유전자 발현 데이터에 대한 간단한 잠재 확산 모델(Latent Diffusion Model)을 구현한다.</p>

<h3 id="31-환경-설정">3.1 환경 설정</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">import</span> <span class="n">scanpy</span> <span class="k">as</span> <span class="n">sc</span>
<span class="kn">import</span> <span class="n">anndata</span> <span class="k">as</span> <span class="n">ad</span>
</code></pre></div></div>

<h3 id="32-데이터-전처리">3.2 데이터 전처리</h3>

<p>scRNA-seq 데이터를 확산 모델에 적합한 형태로 전처리한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_scrna</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">n_top_genes</span><span class="o">=</span><span class="mi">2000</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">scRNA-seq 데이터 전처리 파이프라인</span><span class="sh">"""</span>
    <span class="c1"># 기본 필터링
</span>    <span class="n">sc</span><span class="p">.</span><span class="n">pp</span><span class="p">.</span><span class="nf">filter_cells</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">min_genes</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">sc</span><span class="p">.</span><span class="n">pp</span><span class="p">.</span><span class="nf">filter_genes</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">min_cells</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="c1"># 정규화 및 로그 변환
</span>    <span class="n">sc</span><span class="p">.</span><span class="n">pp</span><span class="p">.</span><span class="nf">normalize_total</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">target_sum</span><span class="o">=</span><span class="mf">1e4</span><span class="p">)</span>
    <span class="n">sc</span><span class="p">.</span><span class="n">pp</span><span class="p">.</span><span class="nf">log1p</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>

    <span class="c1"># 고변이 유전자 선택 (Highly Variable Genes)
</span>    <span class="n">sc</span><span class="p">.</span><span class="n">pp</span><span class="p">.</span><span class="nf">highly_variable_genes</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">n_top_genes</span><span class="o">=</span><span class="n">n_top_genes</span><span class="p">)</span>
    <span class="n">adata</span> <span class="o">=</span> <span class="n">adata</span><span class="p">[:,</span> <span class="n">adata</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">highly_variable</span><span class="p">].</span><span class="nf">copy</span><span class="p">()</span>

    <span class="c1"># 스케일링 (z-score 정규화)
</span>    <span class="n">sc</span><span class="p">.</span><span class="n">pp</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">adata</span>

<span class="c1"># 예시: PBMC 데이터 로드
# adata = sc.read_h5ad("pbmc_data.h5ad")
# adata = preprocess_scrna(adata)
# X = torch.FloatTensor(adata.X)  # (n_cells, n_genes)
# cell_types = adata.obs['cell_type'].cat.codes.values  # 정수 레이블
</span></code></pre></div></div>

<h3 id="33-오토인코더-autoencoder">3.3 오토인코더 (Autoencoder)</h3>

<p>유전자 발현 데이터를 저차원 잠재 공간으로 압축하는 오토인코더를 먼저 학습한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GeneAutoencoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">유전자 발현 오토인코더 — 고차원 데이터를 잠재 공간으로 압축</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_genes</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># 인코더: n_genes → 512 → 256 → latent_dim
</span>        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_genes</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># 디코더: latent_dim → 256 → 512 → n_genes
</span>        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_genes</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x_recon</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_recon</span><span class="p">,</span> <span class="n">z</span>
</code></pre></div></div>

<h3 id="34-노이즈-스케줄-및-확산-유틸리티">3.4 노이즈 스케줄 및 확산 유틸리티</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DiffusionSchedule</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Cosine 노이즈 스케줄 — 오믹스 데이터에 적합</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.008</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span> <span class="o">=</span> <span class="n">timesteps</span>

        <span class="c1"># Cosine schedule 계산
</span>        <span class="n">steps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">timesteps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">((</span><span class="n">steps</span> <span class="o">/</span> <span class="n">timesteps</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">f</span> <span class="o">/</span> <span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># βₜ 계산 (ᾱₜ로부터)
</span>        <span class="n">betas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">betas</span> <span class="o">=</span> <span class="n">betas</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">betas</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 역방향 과정에 필요한 값들
</span>        <span class="n">self</span><span class="p">.</span><span class="n">sqrt_alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sqrt_one_minus_alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sqrt_recip_alphas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">alphas</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">q_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">순방향 과정: xₜ = √ᾱₜ · x₀ + √(1-ᾱₜ) · ε</span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>

        <span class="n">sqrt_alpha</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">sqrt_alphas_cumprod</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">sqrt_one_minus_alpha</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">sqrt_one_minus_alphas_cumprod</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">sqrt_alpha</span> <span class="o">*</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">sqrt_one_minus_alpha</span> <span class="o">*</span> <span class="n">noise</span>
</code></pre></div></div>

<h3 id="35-디노이징-네트워크">3.5 디노이징 네트워크</h3>

<p>유전자 발현 데이터에 맞는 MLP 기반 디노이징 네트워크를 구현한다. 핵심은 <strong>타임스텝 임베딩</strong>과 <strong>스킵 연결</strong>이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SinusoidalPosEmb</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">사인파 위치 임베딩 — 타임스텝 정보를 벡터로 인코딩</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">device</span>
        <span class="n">half_dim</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="n">emb</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">t</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">float</span><span class="p">()</span> <span class="o">*</span> <span class="n">emb</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">emb</span><span class="p">.</span><span class="nf">sin</span><span class="p">(),</span> <span class="n">emb</span><span class="p">.</span><span class="nf">cos</span><span class="p">()],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResidualMLPBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">스킵 연결이 있는 MLP 블록</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># 타임스텝 조건화를 위한 projection
</span>        <span class="n">self</span><span class="p">.</span><span class="n">time_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t_emb</span><span class="p">):</span>
        <span class="c1"># 타임스텝 정보를 더한 뒤 MLP 통과 + 스킵 연결
</span>        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">time_proj</span><span class="p">(</span><span class="n">t_emb</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OmicsDenoiser</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">오믹스 데이터용 디노이징 네트워크

    입력: 노이즈가 추가된 잠재 표현 zₜ + 타임스텝 t
    출력: 예측된 노이즈 ε̂
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                 <span class="n">n_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># 타임스텝 임베딩
</span>        <span class="n">self</span><span class="p">.</span><span class="n">time_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">SinusoidalPosEmb</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="c1"># 조건부 생성: 세포 유형 임베딩 (선택)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">class_emb</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">class_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="p">)</span>

        <span class="c1"># 입력 projection
</span>        <span class="n">self</span><span class="p">.</span><span class="n">input_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>

        <span class="c1"># Residual MLP 블록 스택
</span>        <span class="n">self</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="nc">ResidualMLPBlock</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># 출력 projection (노이즈 예측)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">output_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">class_labels</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Args:
            z_t: 노이즈가 추가된 잠재 표현 (batch, latent_dim)
            t: 타임스텝 (batch,)
            class_labels: 세포 유형 레이블 (batch,), 선택
        Returns:
            noise_pred: 예측된 노이즈 (batch, latent_dim)
        </span><span class="sh">"""</span>
        <span class="c1"># 타임스텝 + 조건 임베딩
</span>        <span class="n">t_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">time_mlp</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">class_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">class_labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">t_emb</span> <span class="o">=</span> <span class="n">t_emb</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">class_emb</span><span class="p">(</span><span class="n">class_labels</span><span class="p">)</span>

        <span class="c1"># 디노이징
</span>        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">input_proj</span><span class="p">(</span><span class="n">z_t</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">h</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">t_emb</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_proj</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="36-학습-루프">3.6 학습 루프</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_diffusion</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">schedule</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">잠재 확산 모델 학습 루프</span><span class="sh">"""</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">schedule_device</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nf">vars</span><span class="p">(</span><span class="n">schedule</span><span class="p">).</span><span class="nf">items</span><span class="p">()</span>
                       <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)}</span>

    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">autoencoder</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>  <span class="c1"># 오토인코더는 고정
</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">batch_x</span> <span class="o">=</span> <span class="n">batch_x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">batch_labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Step 1: 오토인코더로 잠재 표현 추출
</span>            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
                <span class="n">z0</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>

            <span class="c1"># Step 2: 랜덤 타임스텝 샘플링
</span>            <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">schedule</span><span class="p">.</span><span class="n">timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">z0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span>
                            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Step 3: 노이즈 샘플링 및 순방향 확산
</span>            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">z0</span><span class="p">)</span>
            <span class="n">z_t</span> <span class="o">=</span> <span class="n">schedule</span><span class="p">.</span><span class="nf">q_sample</span><span class="p">(</span><span class="n">z0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

            <span class="c1"># Step 4: 노이즈 예측
</span>            <span class="n">noise_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">class_labels</span><span class="o">=</span><span class="n">batch_labels</span><span class="p">)</span>

            <span class="c1"># Step 5: 손실 계산 (Simple MSE)
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

            <span class="c1"># Step 6: 역전파
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>

        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="37-샘플링-역방향-과정">3.7 샘플링 (역방향 과정)</h3>

<p>학습된 모델로부터 새로운 유전자 발현 프로파일을 생성한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">schedule</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
           <span class="n">class_labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">DDPM 샘플링 — 노이즈에서 유전자 발현 프로파일 생성</span><span class="sh">"""</span>

    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="c1"># 순수 가우시안 노이즈에서 시작
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 역방향 과정: T → 0
</span>    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">schedule</span><span class="p">.</span><span class="n">timesteps</span><span class="p">)):</span>
        <span class="n">t_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,),</span> <span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

        <span class="c1"># 노이즈 예측
</span>        <span class="n">noise_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">,</span> <span class="n">class_labels</span><span class="o">=</span><span class="n">class_labels</span><span class="p">)</span>

        <span class="c1"># μθ(xₜ, t) 계산
</span>        <span class="n">alpha_t</span> <span class="o">=</span> <span class="n">schedule</span><span class="p">.</span><span class="n">alphas</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        <span class="n">alpha_bar_t</span> <span class="o">=</span> <span class="n">schedule</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        <span class="n">beta_t</span> <span class="o">=</span> <span class="n">schedule</span><span class="p">.</span><span class="n">betas</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

        <span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alpha_t</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">z</span> <span class="o">-</span> <span class="p">(</span><span class="n">beta_t</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bar_t</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise_pred</span>
        <span class="p">)</span>

        <span class="c1"># 노이즈 추가 (t &gt; 0일 때만)
</span>        <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">beta_t</span><span class="p">)</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">noise</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">mean</span>

    <span class="c1"># 잠재 표현 → 유전자 발현 프로파일 디코딩
</span>    <span class="n">generated_expression</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">generated_expression</span>
</code></pre></div></div>

<h3 id="38-전체-파이프라인-실행">3.8 전체 파이프라인 실행</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_pipeline</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">전체 파이프라인: 데이터 로드 → 학습 → 생성</span><span class="sh">"""</span>
    <span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

    <span class="c1"># ===== 데이터 준비 (예시) =====
</span>    <span class="c1"># 실제로는 scanpy로 h5ad 파일을 로드
</span>    <span class="n">n_cells</span><span class="p">,</span> <span class="n">n_genes</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">10</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_cells</span><span class="p">,</span> <span class="n">n_genes</span><span class="p">)</span>  <span class="c1"># 실제: adata.X
</span>    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="p">(</span><span class="n">n_cells</span><span class="p">,))</span>  <span class="c1"># 실제: cell_type codes
</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="nc">TensorDataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># ===== Phase 1: 오토인코더 학습 =====
</span>    <span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">autoencoder</span> <span class="o">=</span> <span class="nc">GeneAutoencoder</span><span class="p">(</span><span class="n">n_genes</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">ae_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">autoencoder</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">batch_x</span> <span class="o">=</span> <span class="n">batch_x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">x_recon</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="nf">autoencoder</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">x_recon</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">)</span>
            <span class="n">ae_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">ae_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1"># ===== Phase 2: Diffusion Model 학습 =====
</span>    <span class="n">schedule</span> <span class="o">=</span> <span class="nc">DiffusionSchedule</span><span class="p">(</span><span class="n">timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">OmicsDenoiser</span><span class="p">(</span>
        <span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="n">n_classes</span>
    <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="nf">train_diffusion</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">schedule</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># ===== Phase 3: 조건부 샘플 생성 =====
</span>    <span class="c1"># 특정 세포 유형 (예: 클래스 3)의 세포 100개 생성
</span>    <span class="n">target_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="mi">100</span><span class="p">,),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
    <span class="n">generated</span> <span class="o">=</span> <span class="nf">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">schedule</span><span class="p">,</span>
                       <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">class_labels</span><span class="o">=</span><span class="n">target_labels</span><span class="p">,</span>
                       <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">생성된 데이터 shape: </span><span class="si">{</span><span class="n">generated</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># (100, 2000)
</span>    <span class="k">return</span> <span class="n">generated</span>
</code></pre></div></div>

<hr />

<h2 id="part-4-평가-및-시각화">Part 4: 평가 및 시각화</h2>

<p>생성된 세포 데이터의 품질을 평가하는 주요 지표들:</p>

<h3 id="41-평가-지표">4.1 평가 지표</h3>

<table>
  <thead>
    <tr>
      <th>지표</th>
      <th>설명</th>
      <th>이상적 값</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>MMD</strong> (Maximum Mean Discrepancy)</td>
      <td>실제/생성 데이터 분포 간 거리</td>
      <td>0에 가까울수록 좋음</td>
    </tr>
    <tr>
      <td><strong>SCC</strong> (Spearman Correlation)</td>
      <td>유전자 발현 상관관계</td>
      <td>1에 가까울수록 좋음</td>
    </tr>
    <tr>
      <td><strong>LISI</strong> (Local Inverse Simpson’s Index)</td>
      <td>실제/생성 세포의 혼합도</td>
      <td>높을수록 좋음</td>
    </tr>
    <tr>
      <td><strong>RF AUC</strong> (Random Forest)</td>
      <td>실제/생성 구분 정확도</td>
      <td>0.5에 가까울수록 좋음</td>
    </tr>
    <tr>
      <td><strong>Cell Type Accuracy</strong></td>
      <td>생성 세포의 유형 분류 정확도</td>
      <td>높을수록 좋음</td>
    </tr>
  </tbody>
</table>

<h3 id="42-umap-시각화-예시">4.2 UMAP 시각화 예시</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evaluate_generated</span><span class="p">(</span><span class="n">real_adata</span><span class="p">,</span> <span class="n">generated_tensor</span><span class="p">,</span> <span class="n">cell_type_names</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">생성 데이터 품질 평가 및 UMAP 시각화</span><span class="sh">"""</span>
    <span class="kn">import</span> <span class="n">scanpy</span> <span class="k">as</span> <span class="n">sc</span>

    <span class="c1"># 생성 데이터를 AnnData로 변환
</span>    <span class="n">gen_adata</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nc">AnnData</span><span class="p">(</span>
        <span class="n">X</span><span class="o">=</span><span class="n">generated_tensor</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">(),</span>
        <span class="n">var</span><span class="o">=</span><span class="n">real_adata</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">gen_adata</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">generated</span><span class="sh">'</span>

    <span class="n">real_subset</span> <span class="o">=</span> <span class="n">real_adata</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">real_subset</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">real</span><span class="sh">'</span>

    <span class="c1"># 결합 후 UMAP
</span>    <span class="n">combined</span> <span class="o">=</span> <span class="n">ad</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">real_subset</span><span class="p">,</span> <span class="n">gen_adata</span><span class="p">])</span>
    <span class="n">sc</span><span class="p">.</span><span class="n">pp</span><span class="p">.</span><span class="nf">pca</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
    <span class="n">sc</span><span class="p">.</span><span class="n">pp</span><span class="p">.</span><span class="nf">neighbors</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
    <span class="n">sc</span><span class="p">.</span><span class="n">tl</span><span class="p">.</span><span class="nf">umap</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>

    <span class="c1"># 시각화
</span>    <span class="n">sc</span><span class="p">.</span><span class="n">pl</span><span class="p">.</span><span class="nf">umap</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Real vs Generated Cells</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="정리-및-전망">정리 및 전망</h2>

<h3 id="핵심-요약">핵심 요약</h3>

<ol>
  <li><strong>Diffusion model</strong>은 순방향(노이즈 추가)과 역방향(노이즈 제거) 과정을 통해 데이터를 생성하는 모델로, 단순한 MSE 손실로 학습된다</li>
  <li><strong>오믹스 데이터</strong>에 적용할 때는 잠재 확산 모델(Latent Diffusion) 접근이 효과적이며, 공간 구조가 없으므로 MLP 기반 디노이저를 사용한다</li>
  <li><strong>조건부 생성</strong>을 통해 특정 세포 유형, 조직, 질병 상태의 데이터를 선택적으로 생성할 수 있다</li>
</ol>

<h3 id="향후-발전-방향">향후 발전 방향</h3>

<ul>
  <li><strong>이산 확산 모델</strong>: DCM(2026)이 보여주듯, 유전자 발현 카운트의 이산적 특성을 직접 모델링하면 연속 확산 대비 5배 이상의 성능 향상이 가능하다. 연속 공간 변환 시 발생하는 정보 손실을 원천적으로 방지할 수 있다</li>
  <li><strong>멀티오믹스 확산 모델</strong>: 전사체 + 후성유전체 + 단백질체를 동시에 생성하는 통합 모델. scDiffusion-X로 후속 연구가 진행 중이다</li>
  <li><strong>공간 전사체 확산 모델</strong>: 조직의 공간적 유전자 발현 패턴(spatial transcriptomics)을 생성. Stem(2025)이 H&amp;E 이미지로부터 공간 유전자 발현을 추론하는 시도를 보여주었다</li>
  <li><strong>노화 연구 적용</strong>: 시간에 따른 세포 상태 변화를 Gradient Interpolation으로 모델링, 노화 과정의 연속적 전사체 변화 재구성</li>
  <li><strong>약물 반응 예측</strong>: 섭동(perturbation) 조건부 확산 모델로 약물의 전사체 반응 예측. scVAEDer(2025)가 잠재 공간의 벡터 산술로 섭동 반응을 예측하는 접근을 제시했다</li>
  <li><strong>Flow Matching</strong>: 확산 모델의 느린 생성 속도를 해결하는 차세대 접근법으로, scLDM(2025)이 선형 보간 기반 Flow Matching 손실을 채택하여 더 효율적인 학습과 생성을 달성했다</li>
</ul>

<p>오믹스 데이터의 생성 모델은 2024년 이후 빠르게 발전하고 있다. 확산 모델의 도입으로 생성 품질이 크게 향상되었으며, 파운데이션 모델과의 결합, 이산 확산, Flow Matching 등 새로운 접근법이 잇따르고 있다. 실험 데이터의 한계를 넘는 in silico 생물학의 새로운 장이 열리고 있다.</p>

<hr />

<p><strong>References</strong></p>

<p><em>Diffusion Model 기초 이론</em></p>
<ul>
  <li>Ho, J., Jain, A. &amp; Abbeel, P. Denoising Diffusion Probabilistic Models. <em>NeurIPS</em> (2020). <a href="https://arxiv.org/abs/2006.11239">arXiv:2006.11239</a></li>
  <li>Song, Y. &amp; Ermon, S. Generative Modeling by Estimating Gradients of the Data Distribution. <em>NeurIPS</em> (2019). <a href="https://arxiv.org/abs/1907.05600">arXiv:1907.05600</a></li>
  <li>Song, Y. et al. Score-Based Generative Modeling through Stochastic Differential Equations. <em>ICLR</em> (2021). <a href="https://arxiv.org/abs/2011.13456">arXiv:2011.13456</a></li>
  <li>Nichol, A. &amp; Dhariwal, P. Improved Denoising Diffusion Probabilistic Models. <em>ICML</em> (2021). <a href="https://arxiv.org/abs/2102.09672">arXiv:2102.09672</a></li>
  <li>Rombach, R. et al. High-Resolution Image Synthesis with Latent Diffusion Models. <em>CVPR</em> (2022). <a href="https://arxiv.org/abs/2112.10752">arXiv:2112.10752</a></li>
  <li>Ho, J. &amp; Salimans, T. Classifier-Free Diffusion Guidance. <em>NeurIPS Workshop</em> (2022). <a href="https://arxiv.org/abs/2207.12598">arXiv:2207.12598</a></li>
</ul>

<p><em>오믹스 Diffusion Model</em></p>
<ul>
  <li>Luo, E. et al. scDiffusion: conditional generation of high-quality single-cell data using diffusion model. <em>Bioinformatics</em> <strong>40</strong>, btae518 (2024). <a href="https://doi.org/10.1093/bioinformatics/btae518">DOI</a></li>
  <li>Zhang, H. et al. cfDiffusion: Conditional generation of high-quality single-cell data via classifier-free guidance. <em>Brief. Bioinform.</em> <strong>26</strong>, bbaf071 (2025). <a href="https://doi.org/10.1093/bib/bbaf071">DOI</a></li>
  <li>Gao, H. et al. scVAEDer: Integrating deep diffusion models and variational autoencoders for single-cell transcriptomics analysis. <em>Genome Biol.</em> <strong>26</strong>, 60 (2025). <a href="https://doi.org/10.1186/s13059-025-03519-4">DOI</a></li>
  <li>scLDM: A Scalable Latent Diffusion Model for single-cell data. <em>arXiv</em> (2025). <a href="https://arxiv.org/abs/2511.02986">arXiv:2511.02986</a></li>
  <li>Guo, Z. et al. Diffusion models in bioinformatics and computational biology. <em>Nat. Rev. Bioeng.</em> <strong>2</strong>, 136–154 (2024). <a href="https://doi.org/10.1038/s44222-023-00114-9">DOI</a></li>
</ul>

  </div>

  <footer class="post-footer">
    <div class="post-nav">
      
      <a class="post-nav__prev" href="/research/2026/02/25/plasma-proteomics-platform-comparison/">
        <span class="post-nav__label">← 이전 글</span>
        <span class="post-nav__title">혈장 프로테오믹스의 현재: 8개 플랫폼 비교를 통한 기술적 혁신과 바이오마커 발견</span>
      </a>
      
      
    </div>
  </footer>
   <!-- 댓글 (giscus) -->
  
   <!-- 댓글 영역 -->
<div class="comments-section">
    <h2 class="comments-title">💬 댓글</h2>
    <script src="https://giscus.app/client.js"
    data-repo="jsungwon/jsungwon.github.io"
    data-repo-id="R_kgDORQmu8A"
    data-category="Announcements"
    data-category-id="DIC_kwDORQmu8M4C2eY7"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="preferred_color_scheme"
    data-lang="ko"
    crossorigin="anonymous"
    async>
</script>
  </div>
  
  <style>
    .comments-section {
      max-width: 720px;
      margin: 3rem auto 0;
      padding-top: 2rem;
      border-top: 1px solid #e8e4df;
    }
    .comments-title {
      font-family: 'DM Serif Display', Georgia, serif;
      font-size: 1.3rem;
      margin-bottom: 1.5rem;
      letter-spacing: -0.02em;
    }
  </style>
  
 
</article>

  </main>

  <footer class="site-footer">
  <div class="footer-inner">
    <div class="footer-brand">
      <svg class="logo-svg logo-svg--footer" width="30" height="30" viewBox="0 0 28 28" fill="none" aria-hidden="true">
        <path d="M9 2C20 5 20 10 14 14C8 18 8 23 19 26" stroke="#2563eb" stroke-width="2.5" stroke-linecap="round" fill="none"/>
        <path d="M19 2C8 5 8 10 14 14C20 18 20 23 9 26" stroke="#7c3aed" stroke-width="2.5" stroke-linecap="round" fill="none"/>
        <line x1="11" y1="6" x2="17" y2="6" stroke="#93c5fd" stroke-width="1.5" stroke-linecap="round"/>
        <line x1="12" y1="10" x2="16" y2="10" stroke="#c4b5fd" stroke-width="1.5" stroke-linecap="round"/>
        <line x1="12" y1="18" x2="16" y2="18" stroke="#93c5fd" stroke-width="1.5" stroke-linecap="round"/>
        <line x1="11" y1="22" x2="17" y2="22" stroke="#c4b5fd" stroke-width="1.5" stroke-linecap="round"/>
      </svg>
      <p>Bioinformatics, Tech, Travel & Life</p>
    </div>
    <div class="footer-links">
      
      <a href="https://github.com/jsungwon" target="_blank" rel="noopener">GitHub</a>
      
      
      <a href="mailto:jsw0061@gmail.com">Email</a>
      
    </div>
    <div class="footer-copy">
      <p>&copy; 2026 Sungwon. Built with Jekyll & GitHub Pages.</p>
    </div>
  </div>
</footer>

</body>
</html>
